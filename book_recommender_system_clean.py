# -*- coding: utf-8 -*-
"""book_recommender_system_clean.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ix9SgdmZVqvFN9XDcWaTOR9npPPuXj4R

# Requirements
"""

import numpy as np
import pandas as pd

"""# Ratings/ Recommender system

## Data Test
"""

#r"C:\Users\Hugo\Documents\Travail\A4\Chatbot & Recommandation systems\Project_books\reco_discord"

goodbooks_data = pd.read_csv("books.csv")
goodbooks_data.head()

goodbooks_data.dropna(subset=["isbn13"],inplace=True)
goodbooks_data.reset_index(inplace=True,drop=True)

goodbooks_data.isbn13 = [int(i) for i in goodbooks_data.isbn13]

"""## Data Preparation"""

goodbooks_ratings = pd.read_csv("ratings.csv")

"""selecting only a small portion of users and books to gain performance (might change the parameters when the clustering will work well)"""

user_sample = list(goodbooks_ratings.user_id.value_counts().index[:5000])
book_sample = list(goodbooks_ratings.book_id.value_counts().index[:5000])

ratings_sample = goodbooks_ratings[goodbooks_ratings.user_id.isin(user_sample)]
ratings_sample = ratings_sample[ratings_sample.book_id.isin(book_sample)]

ratings_sample.reset_index(drop=True,inplace=True)

user_new_index = [i for i in range(len(ratings_sample.user_id.value_counts()))]
book_new_index = [i for i in range(len(ratings_sample.book_id.value_counts()))]
dict_user = dict(zip(user_sample,user_new_index))
dict_book = dict(zip(book_sample,book_new_index))

inv_dict_book = {v: k for k, v in dict_book.items()}
inv_dict_user = {v: k for k, v in dict_user.items()}

"""## Clustering Method"""

utility_matrix_big = pd.read_csv("utility_matrix_5.csv")

utility_matrix_big.drop("Unnamed: 0",axis=1,inplace=True)

matrix_df = pd.read_csv("utility_matrix_5.csv")
matrix_df.drop("Unnamed: 0",axis=1,inplace=True)

import pickle
# load the model
model = pickle.load(open("model.pkl", "rb"))

cluster_user = pd.DataFrame({"user" : list(matrix_df.index),"cluster" : model.labels_}).groupby("cluster")

def norm(vector):
  result = 0 
  for i in (vector.index):
    result = result + vector[i]**2
  result = np.sqrt(result)
  return result

def cos(c,m):
  return np.dot(c,m)/(norm(c)*norm(m))


def prediction_cluster(book,data,n,sim_users):
  sum_rankings = 0.0
  try: 
    for i in sim_users:
      sum_rankings+=data.iloc[i][book]
  except Exception as e:
    print(e)
    print("user : ",i)
    print("book :",book)
  return sum_rankings/n


def find_similar_user_cluster(user,data,n,data_user):
  similar_user = []
  top_user=[]
  for j in range(len(data)-1):
    try:

      if j!=user:
        cos(data.iloc[j],data_user)
        similar_user.append([j,cos(data_user,data.iloc[j])])
    except Exception as e:
      print(e)
      print(j)
  top_user=sorted(similar_user, key=lambda x:x[1],reverse=True)[:n]
  return [i[0] for i in top_user]


def recommendation_cluster(user,matrix,n,inv_dict_book,goodbooks_data,model_cluster,clustered_user):
  print("matrix: before :", matrix.shape)
  print("user : ",user)
  recommend_books=[]

  #data about the user inside the utility matrix
  user_vector =  matrix.iloc[user].values
  
  ## CLUSTERING
  #find the closest cluster to our user
  cluster_user_n = model_cluster.predict(user_vector.reshape(1,-1))
  print("belongs to cluster :",cluster_user_n)
  #individuals inside the cluster
  list_user = clustered_user.get_group(cluster_user_n[0]).user.values
  #only keep a small matrixes
  matrix =  matrix.loc[list_user]
  print("cluster of :",matrix.shape[0]," individuals")

  ##FIND SIMILAR USERS WITHIN THE CLUSTER
  sim_users = find_similar_user_cluster(user,matrix,n,pd.Series(user_vector))
  print("sim_user : ", sim_users)

  ## DROP ALREADY READ BOOKS
  #we dont want to recommend a book that our user already read, so we modify our matrix 
  #we assume the minimum note in the website is 1, so a 0 corresponds to smth not read
  history_user_n = matrix_df.iloc[user]
  already_read = []
  for i in history_user_n.index:
    if history_user_n[i]!=0.0:
      already_read.append(i)
  print("user read : ",already_read)
  
  #we want to only keep the columns of the book that the user did not read
  matrix.drop(already_read,axis=1,inplace=True)

  ##RECO NEW BOOKS THANKS TO THE CLOSEST USERS
  print("matrix after : ", matrix.shape)

  ## get rid of books that none of the people inside the clusters have read
  no_read =[i for i in matrix.columns if matrix[i].sum()!=0.0 ]
  matrix = matrix[no_read]
  print("cluster of :",matrix.shape[1],"books")

  for books in matrix.columns:
    recommend_books.append([books,prediction_cluster(books,matrix,n,sim_users)])
  top_reco_id= [i[0] for i in sorted(recommend_books, key=lambda x:x[1],reverse=True)[:n]] 
  print(sorted(recommend_books, key=lambda x:x[1],reverse=True)[:n]) 
  reco_data = goodbooks_data.loc[goodbooks_data.book_id.isin([inv_dict_book[int(i)] for i in top_reco_id])]
  return reco_data

print(recommendation_cluster(586,matrix_df,5,inv_dict_book,goodbooks_data,model,cluster_user))